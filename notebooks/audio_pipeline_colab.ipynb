{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e539a2a",
   "metadata": {},
   "source": [
    "# IRIS Audio Pipeline (Colab GPU)\n",
    "\n",
    "Notebook to exercise the audio pipeline on a Colab GPU runtime. It installs GPU-ready deps, loads the IRIS code, and runs a sample window over a WAV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248302a",
   "metadata": {},
   "source": [
    "**Prep**\n",
    "- Runtime ➜ Change runtime type ➜ GPU\n",
    "- Place this repo at `/content/IRIS` (clone or upload). The pipeline code lives in `poc/src` and sample audio in `poc/data/audio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU is visible\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print('torch', torch.__version__)\n",
    "print('cuda available', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481be6c4",
   "metadata": {},
   "source": [
    "Install dependencies (CUDA wheel picked automatically from nvidia-smi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "CUDA_VARIANT = 'cu121'  # fallback\n",
    "try:\n",
    "    smi = subprocess.check_output(['nvidia-smi']).decode()\n",
    "    if 'CUDA Version: 11.' in smi:\n",
    "        CUDA_VARIANT = 'cu118'\n",
    "    elif 'CUDA Version: 12.' in smi:\n",
    "        CUDA_VARIANT = 'cu121'\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    print('Could not auto-detect CUDA version, defaulting to cu121:', exc)\n",
    "\n",
    "print(f'Using CUDA wheels: {CUDA_VARIANT}')\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --extra-index-url https://download.pytorch.org/whl/$CUDA_VARIANT torch torchaudio\n",
    "!pip install -q -f https://opennmt.net/CTranslate2/whl/$CUDA_VARIANT ctranslate2==4.3.1 faster-whisper==1.0.3\n",
    "!pip install -q silero-vad speechbrain praat-parselmouth sentence-transformers soundfile librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc9e5b",
   "metadata": {},
   "source": [
    "**Get the code**\n",
    "- If the repo is public, set `REPO_URL` to clone.\n",
    "- Otherwise upload/drag the `IRIS` folder into the Colab file pane or mount Drive to `/content/IRIS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4572c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = ''  # e.g., 'https://github.com/.../IRIS.git'\n",
    "if REPO_URL:\n",
    "    !rm -rf /content/IRIS\n",
    "    !git clone $REPO_URL /content/IRIS\n",
    "else:\n",
    "    print('Upload or mount the IRIS repo to /content/IRIS (with poc/src and poc/data/audio).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path('/content/IRIS')\n",
    "CODE_ROOT = PROJECT_ROOT / 'poc' / 'src'\n",
    "if not CODE_ROOT.exists():\n",
    "    raise FileNotFoundError('IRIS code not found. Ensure /content/IRIS/poc/src exists (see previous cell).')\n",
    "\n",
    "sys.path.append(str(CODE_ROOT))\n",
    "print('Using code from', CODE_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877363c0",
   "metadata": {},
   "source": [
    "Load a sample WAV (replace with your own if you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "AUDIO_FILE = PROJECT_ROOT / 'poc' / 'data' / 'audio' / 'slice2_audio.wav'\n",
    "if not AUDIO_FILE.exists():\n",
    "    raise FileNotFoundError('Audio file not found. Point AUDIO_FILE to a WAV you uploaded.')\n",
    "\n",
    "audio, sr = sf.read(AUDIO_FILE, dtype='float32')\n",
    "if audio.ndim > 1:\n",
    "    audio = audio.mean(axis=1)  # mono\n",
    "\n",
    "TARGET_SR = 16000\n",
    "if sr != TARGET_SR:\n",
    "    import librosa\n",
    "    audio = librosa.resample(audio, orig_sr=sr, target_sr=TARGET_SR)\n",
    "    sr = TARGET_SR\n",
    "\n",
    "print(f'Loaded {AUDIO_FILE.name}: sr={sr}, shape={audio.shape}, duration={len(audio)/sr:.2f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e518c6c",
   "metadata": {},
   "source": [
    "Run the pipeline over the window. Switch `diarization_backend` to `'simple'` if you want a faster lightweight diarizer. Set `enable_prosody=True` to test prosody (adds compute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from audio.pipeline import AudioPipeline, PipelineConfig\n",
    "\n",
    "config = PipelineConfig(\n",
    "    debug=True,\n",
    "    diarization_backend='speechbrain',  # use 'simple' for fastest smoke test\n",
    "    asr_model='ivrit-v2-d4',            # Hebrew optimized faster-whisper\n",
    "    enable_prosody=False,               # enable to exercise prosody extractor\n",
    "    retain_transcripts=True,\n",
    ")\n",
    "\n",
    "pipeline = AudioPipeline(config)\n",
    "result = pipeline.process_window(audio, datetime.now())\n",
    "\n",
    "print(f'Processing time: {result.processing_time_ms} ms')\n",
    "print(f'Utterances: {result.num_utterances}, Speakers detected: {result.speakers_detected}')\n",
    "print('--- Sample events ---')\n",
    "for evt in result.speech_events[:5]:\n",
    "    print(evt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249656ce",
   "metadata": {},
   "source": [
    "Optional: tabular view of events (keeps transcripts only because retain_transcripts=True above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8540ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for evt in result.speech_events:\n",
    "    rows.append({\n",
    "        'timestamp': evt.timestamp,\n",
    "        'speaker': getattr(evt.speaker, 'id', None),\n",
    "        'duration_ms': evt.duration_ms,\n",
    "        'text': getattr(evt, 'transcription', None),\n",
    "        'complexity': getattr(evt, 'complexity', None),\n",
    "        'gap_before_ms': evt.gap_before_ms,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
