"""
IRIS Event Schema v3.0

Two-layer architecture for privacy-preserving classroom observation:

LAYER 1 - Raw Sensor Events (from ASR/CV models):
  Audio-derived:
    - SpeechEvent: transcription, prosody, speaker ID
    - AmbientAudioEvent: environmental sounds
  Video-derived:
    - ProximityEvent: spatial relationships
    - GazeEvent: attention direction
    - PostureEvent: body position/movement
    - ObjectEvent: object interactions

LAYER 2 - LLM-Inferred Events (compound/semantic):
    - BehavioralEvent: interpreted behaviors with triggers
    - InteractionEvent: social meaning of exchanges
    - ContextEvent: classroom state and climate

All actors identified by anonymous IDs only - no PII stored.
"""

from __future__ import annotations

from datetime import datetime
from enum import Enum
from typing import Optional, List, Union
from dataclasses import dataclass, field

# =============================================================================
# COMMON ENUMS
# =============================================================================

class ActorRole(str, Enum):
    CHILD = "child"
    ADULT = "adult"  # teacher, aide, therapist


class ClassroomZone(str, Enum):
    """Physical zones in classroom."""
    CIRCLE_AREA = "circle_area"
    WORK_TABLES = "work_tables"
    SENSORY_CORNER = "sensory_corner"
    PLAY_AREA = "play_area"
    QUIET_CORNER = "quiet_corner"
    ENTRANCE = "entrance"
    BATHROOM_AREA = "bathroom_area"
    TEACHER_DESK = "teacher_desk"
    UNKNOWN = "unknown"


class Intensity(str, Enum):
    """Intensity level for behaviors."""
    LOW = "low"
    MODERATE = "moderate"
    HIGH = "high"


# =============================================================================
# CORE TYPES
# =============================================================================

@dataclass
class Actor:
    """Anonymous identifier for a person in the classroom."""
    id: str                    # e.g., "child_1", "adult_2"
    role: ActorRole
    
    def __hash__(self):
        return hash(self.id)
    
    def __eq__(self, other):
        if isinstance(other, Actor):
            return self.id == other.id
        return False


@dataclass
class Location:
    """Spatial location in classroom."""
    zone: ClassroomZone
    x: Optional[float] = None  # normalized 0-1
    y: Optional[float] = None  # normalized 0-1


@dataclass 
class EventBase:
    """Base class for all events."""
    event_id: str
    timestamp: datetime
    
    def __post_init__(self):
        if not self.event_id:
            import uuid
            self.event_id = str(uuid.uuid4())[:8]


# =============================================================================
# LAYER 1: AUDIO-DERIVED EVENTS
# =============================================================================
@dataclass
class SpeechSegment:
    """Segment of detected speech in audio stream."""
    start_time: datetime
    end_time: datetime
    speaker_id: str  # temporary ID before mapping to Actor

class SpeechTarget(str, Enum):
    """Who speech is directed at (inferred from audio direction/context)."""
    PEER = "peer"
    ADULT = "adult"
    SELF = "self"
    BROADCAST = "broadcast"
    UNKNOWN = "unknown"


class VocalType(str, Enum):
    """Type of vocalization detected."""
    SPEECH = "speech"              # intelligible words
    VOCALIZATION = "vocalization"  # non-word sounds
    CRY = "cry"
    LAUGH = "laugh"
    SCREAM = "scream"
    HUM = "hum"
    WHISPER = "whisper"


class VerbalComplexity(str, Enum):
    """Complexity of speech (from ASR word count)."""
    VOCALIZATION = "vocalization"  # non-word
    SINGLE_WORD = "single_word"
    PHRASE = "phrase"              # 2-3 words
    SENTENCE = "sentence"          # 4+ words
    MULTI_SENTENCE = "multi_sentence"


@dataclass
class ProsodyFeatures:
    """Acoustic features extracted from speech segment."""
    pitch_mean_hz: Optional[float] = None
    pitch_std_hz: Optional[float] = None
    pitch_contour: Optional[str] = None  # "rising", "falling", "flat", "variable"
    
    intensity_mean_db: Optional[float] = None
    intensity_range_db: Optional[float] = None
    
    speech_rate: Optional[str] = None    # "slow", "normal", "fast", "variable"
    rhythm_regularity: Optional[float] = None  # 0-1, how regular the rhythm
    
    voice_quality: Optional[str] = None  # "breathy", "tense", "modal", "creaky"


@dataclass
class SpeechEvent(EventBase):
    """
    LAYER 1 - Audio derived
    
    Raw speech detection with transcription and prosody.
    Generated by ASR model (ivrit-ai/faster-whisper) + prosody extraction.
    """
    speaker: Actor
    
    # What was said (Hebrew transcription)
    transcription: Optional[str] = None  # Will be discarded after LLM processing
    word_count: int = 0
    complexity: VerbalComplexity = VerbalComplexity.VOCALIZATION
    
    # Type and target
    vocal_type: VocalType = VocalType.SPEECH
    target: SpeechTarget = SpeechTarget.UNKNOWN
    
    # Prosody
    prosody: Optional[ProsodyFeatures] = None
    duration_ms: int = 0
    
    # Turn-taking context
    gap_before_ms: Optional[int] = None  # silence before this utterance
    is_overlap: bool = False             # overlapping with another speaker
    previous_speaker: Optional[str] = None
    
    # Pattern flags (from embedding similarity)
    is_echolalia_candidate: bool = False
    echolalia_similarity: Optional[float] = None  # 0-1 similarity to recent utterance
    echolalia_delay_ms: Optional[int] = None      # time since echoed utterance
    
    is_perseveration_candidate: bool = False  # repeated topic/phrase pattern


@dataclass
class AmbientAudioEvent(EventBase):
    """
    LAYER 1 - Audio derived
    
    Environmental sounds detected (not speech).
    Generated by audio event detection model.
    """
    sound_type: str  # "bell", "door", "chair_scrape", "crash", "music", "drilling", etc.
    intensity: Intensity = Intensity.MODERATE
    duration_ms: int = 0
    location_estimate: Optional[ClassroomZone] = None


# =============================================================================
# LAYER 1: VIDEO-DERIVED EVENTS
# =============================================================================

class ProximityChange(str, Enum):
    """Type of proximity change detected."""
    APPROACH = "approach"
    WITHDRAWAL = "withdrawal"
    STABLE = "stable"


class ProximityLevel(str, Enum):
    """Closeness between actors."""
    INTIMATE = "intimate"      # < 0.5m, touching distance
    PERSONAL = "personal"      # 0.5-1.2m, conversation distance  
    SOCIAL = "social"          # 1.2-3.5m, group interaction
    PUBLIC = "public"          # > 3.5m, across room


@dataclass
class ProximityEvent(EventBase):
    """
    LAYER 1 - Video derived
    
    Spatial relationship change between actors.
    Generated by pose estimation / tracking model.
    """
    actor: Actor
    target: Actor  # who they're moving toward/away from
    
    change_type: ProximityChange
    proximity_level: ProximityLevel
    
    distance_meters: Optional[float] = None
    movement_speed: Optional[str] = None  # "slow", "normal", "fast", "abrupt"
    
    actor_location: Optional[Location] = None


class GazeDirection(str, Enum):
    """Where gaze is directed."""
    AT_PERSON = "at_person"
    AT_OBJECT = "at_object"
    AT_ACTIVITY = "at_activity"
    AVERTED = "averted"
    DOWNCAST = "downcast"
    SCANNING = "scanning"
    UNFOCUSED = "unfocused"


@dataclass
class GazeEvent(EventBase):
    """
    LAYER 1 - Video derived
    
    Gaze direction and attention target.
    Generated by gaze estimation model.
    """
    actor: Actor
    
    direction: GazeDirection
    target_actor: Optional[Actor] = None    # if looking at person
    target_object: Optional[str] = None     # if looking at object
    target_zone: Optional[ClassroomZone] = None
    
    duration_ms: int = 0
    is_mutual: bool = False  # eye contact with another person
    
    # Quality indicators
    is_fleeting: bool = False   # very brief glance
    is_sustained: bool = False  # held gaze


class PostureType(str, Enum):
    """Body posture category."""
    STANDING = "standing"
    SITTING = "sitting"
    KNEELING = "kneeling"
    LYING = "lying"
    CROUCHING = "crouching"


class MovementType(str, Enum):
    """Type of body movement."""
    STILL = "still"
    WALKING = "walking"
    RUNNING = "running"
    JUMPING = "jumping"
    ROCKING = "rocking"
    PACING = "pacing"
    HAND_FLAPPING = "hand_flapping"
    SPINNING = "spinning"
    FIDGETING = "fidgeting"
    REACHING = "reaching"
    POINTING = "pointing"


class BodyOrientation(str, Enum):
    """Body facing direction relative to group/activity."""
    ENGAGED = "engaged"        # facing activity/group
    PARTIALLY_TURNED = "partially_turned"
    TURNED_AWAY = "turned_away"
    BACK_TO_GROUP = "back_to_group"


@dataclass
class PostureEvent(EventBase):
    """
    LAYER 1 - Video derived
    
    Body position and movement detection.
    Generated by pose estimation model.
    """
    actor: Actor
    
    posture: PostureType
    movement: MovementType = MovementType.STILL
    orientation: BodyOrientation = BodyOrientation.ENGAGED
    
    location: Optional[Location] = None
    
    # Movement details
    movement_intensity: Intensity = Intensity.LOW
    is_repetitive: bool = False  # same movement repeating
    repetition_frequency: Optional[float] = None  # Hz if repetitive


class ObjectAction(str, Enum):
    """Type of object interaction."""
    PICK_UP = "pick_up"
    PUT_DOWN = "put_down"
    HOLD = "hold"
    MANIPULATE = "manipulate"
    THROW = "throw"
    PUSH = "push"
    GIVE = "give"
    RECEIVE = "receive"
    POINT_AT = "point_at"


@dataclass
class ObjectEvent(EventBase):
    """
    LAYER 1 - Video derived
    
    Object interaction detection.
    Generated by object detection + tracking model.
    """
    actor: Actor
    
    object_type: str  # "toy", "book", "pencil", "fidget", "food", etc.
    action: ObjectAction
    
    # Context
    is_appropriate: Optional[bool] = None  # appropriate use of object
    shared_with: Optional[Actor] = None    # if giving/sharing


# =============================================================================
# LAYER 2: LLM-INFERRED EVENTS (Compound/Semantic)
# =============================================================================

class ContentType(str, Enum):
    """Semantic content type of speech (LLM inferred)."""
    REQUEST = "request"
    PROTEST = "protest"
    COMMENT = "comment"
    RESPONSE = "response"
    QUESTION = "question"
    GREETING = "greeting"
    ECHOLALIA_IMMEDIATE = "echolalia_immediate"
    ECHOLALIA_DELAYED = "echolalia_delayed"
    PERSEVERATION = "perseveration"
    SELF_TALK = "self_talk"
    SCRIPTED = "scripted"  # memorized phrases
    OTHER = "other"


class CommunicativeIntent(str, Enum):
    """Underlying intent of communication (LLM inferred)."""
    SOCIAL_CONNECTION = "social_connection"
    INFORMATION_SEEKING = "information_seeking"
    INFORMATION_SHARING = "information_sharing"
    EMOTIONAL_EXPRESSION = "emotional_expression"
    SELF_REGULATION = "self_regulation"
    ATTENTION_SEEKING = "attention_seeking"
    NEED_EXPRESSION = "need_expression"
    PLAY_INITIATION = "play_initiation"
    UNCLEAR = "unclear"


class BehaviorCategory(str, Enum):
    """High-level behavior category (LLM inferred)."""
    STIMMING = "stimming"
    SELF_REGULATION = "self_regulation"
    SENSORY_SEEKING = "sensory_seeking"
    SENSORY_AVOIDANCE = "sensory_avoidance"
    EMOTIONAL_EXPRESSION = "emotional_expression"
    SOCIAL_APPROACH = "social_approach"
    SOCIAL_WITHDRAWAL = "social_withdrawal"
    TASK_ENGAGEMENT = "task_engagement"
    TASK_AVOIDANCE = "task_avoidance"
    DISTRESS = "distress"
    AGGRESSION = "aggression"
    COMPLIANCE = "compliance"
    NONCOMPLIANCE = "noncompliance"


class EmotionalState(str, Enum):
    """Apparent emotional state (LLM inferred from multiple cues)."""
    CALM = "calm"
    HAPPY = "happy"
    EXCITED = "excited"
    ANXIOUS = "anxious"
    FRUSTRATED = "frustrated"
    SAD = "sad"
    ANGRY = "angry"
    OVERWHELMED = "overwhelmed"
    WITHDRAWN = "withdrawn"
    NEUTRAL = "neutral"
    UNCLEAR = "unclear"


class TriggerType(str, Enum):
    """What triggered a behavior (LLM inferred)."""
    SENSORY_INPUT = "sensory_input"
    SOCIAL_DEMAND = "social_demand"
    TRANSITION = "transition"
    FRUSTRATION = "frustration"
    ANTICIPATION = "anticipation"
    UNMET_NEED = "unmet_need"
    PEER_ACTION = "peer_action"
    ADULT_ACTION = "adult_action"
    ENVIRONMENTAL = "environmental"
    INTERNAL = "internal"
    UNKNOWN = "unknown"


@dataclass
class BehavioralEvent(EventBase):
    """
    LAYER 2 - LLM inferred
    
    Interpreted behavior with semantic meaning.
    Inferred from combination of Layer 1 events.
    
    Source events might include:
    - PostureEvent (rocking movement)
    - GazeEvent (averted gaze)
    - ProximityEvent (withdrawal)
    → LLM infers: self-regulation behavior due to sensory overload
    """
    actor: Actor
    
    # What behavior
    category: BehaviorCategory
    description: str  # brief description: "covering ears and rocking"
    
    # Intensity and state
    intensity: Intensity = Intensity.MODERATE
    apparent_emotion: EmotionalState = EmotionalState.UNCLEAR
    
    # Why (LLM reasoning)
    trigger: TriggerType = TriggerType.UNKNOWN
    trigger_description: Optional[str] = None  # "loud noise from hallway"
    
    # Effectiveness (if self-regulation)
    regulation_effective: Optional[bool] = None
    
    # Source events that led to this inference
    source_event_ids: List[str] = field(default_factory=list)
    confidence: float = 0.8  # LLM confidence in inference


class InteractionType(str, Enum):
    """Type of social interaction (LLM inferred)."""
    INITIATION = "initiation"
    RESPONSE = "response"
    JOINT_ATTENTION = "joint_attention"
    PARALLEL_PLAY = "parallel_play"
    COOPERATIVE_PLAY = "cooperative_play"
    HELP_SEEKING = "help_seeking"
    HELP_GIVING = "help_giving"
    CONFLICT = "conflict"
    REPAIR = "repair"
    IGNORE = "ignore"
    REJECTION = "rejection"


class InteractionQuality(str, Enum):
    """Quality/outcome of interaction (LLM inferred)."""
    SUCCESSFUL = "successful"
    PARTIAL = "partial"
    UNSUCCESSFUL = "unsuccessful"
    ONGOING = "ongoing"


@dataclass
class InteractionEvent(EventBase):
    """
    LAYER 2 - LLM inferred
    
    Social interaction with semantic meaning.
    Inferred from combination of speech, gaze, proximity events.
    
    Source events might include:
    - SpeechEvent (verbal initiation)
    - GazeEvent (eye contact attempt)
    - ProximityEvent (approach)
    → LLM infers: social initiation attempt, partially successful
    """
    initiator: Actor
    recipient: Actor
    
    interaction_type: InteractionType
    description: str  # "child_1 invited child_2 to play with blocks"
    
    # Quality assessment
    quality: InteractionQuality = InteractionQuality.ONGOING
    reciprocity_level: Optional[str] = None  # "none", "minimal", "moderate", "full"
    
    # Communication details
    content_type: Optional[ContentType] = None  # if speech involved
    communicative_intent: Optional[CommunicativeIntent] = None
    
    # Adult involvement
    adult_facilitated: bool = False
    adult_actor: Optional[Actor] = None
    
    # Source events
    source_event_ids: List[str] = field(default_factory=list)
    confidence: float = 0.8


class ActivityType(str, Enum):
    """Classroom activity type."""
    CIRCLE_TIME = "circle_time"
    FREE_PLAY = "free_play"
    STRUCTURED_ACTIVITY = "structured_activity"
    SNACK_TIME = "snack_time"
    OUTDOOR_PLAY = "outdoor_play"
    TRANSITION = "transition"
    ONE_ON_ONE = "one_on_one"
    SMALL_GROUP = "small_group"
    CLEANUP = "cleanup"
    ARRIVAL = "arrival"
    DEPARTURE = "departure"
    SENSORY_BREAK = "sensory_break"


class ClassroomClimate(str, Enum):
    """Overall classroom atmosphere (LLM inferred)."""
    CALM = "calm"
    FOCUSED = "focused"
    ENERGETIC = "energetic"
    RESTLESS = "restless"
    CHAOTIC = "chaotic"
    TENSE = "tense"


@dataclass
class ContextEvent(EventBase):
    """
    LAYER 2 - LLM inferred
    
    Classroom-level context and climate.
    Inferred from aggregate of all Layer 1 events.
    
    Generated periodically (e.g., every 30-60 seconds) to track
    overall classroom state.
    """
    # Activity
    activity_type: ActivityType
    activity_description: Optional[str] = None
    
    # Location focus
    primary_zone: ClassroomZone = ClassroomZone.UNKNOWN
    
    # Climate
    classroom_climate: ClassroomClimate = ClassroomClimate.CALM
    noise_level: Intensity = Intensity.MODERATE
    
    # Transitions
    is_transition: bool = False
    transition_from: Optional[ActivityType] = None
    transition_to: Optional[ActivityType] = None
    
    # Adult positioning
    adults_present: int = 1
    adult_attention_focus: Optional[str] = None  # "whole group", "child_3", etc.
    
    # Source events
    source_event_ids: List[str] = field(default_factory=list)
    confidence: float = 0.8


# =============================================================================
# SESSION CONTAINER
# =============================================================================

@dataclass
class SessionMetadata:
    """Metadata about a recording session."""
    session_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    scenario_name: Optional[str] = None
    scenario_description: Optional[str] = None
    
    # Classroom info
    num_children: int = 0
    num_adults: int = 0
    
    # Processing info
    layer1_llm_model: Optional[str] = None  # LLM model used to simulate L1 events
    layer1_model_versions: Optional[dict] = None  # {"asr": "...", "pose": "..."}
    layer2_llm_model: Optional[str] = None
    reconstruction_llm_model: Optional[str] = None
    language: str = "en"


@dataclass
class Session:
    """
    Complete session with both event layers.
    """
    metadata: SessionMetadata
    actors: List[Actor] = field(default_factory=list)
    
    # Layer 1 events (raw sensor)
    speech_events: List[SpeechEvent] = field(default_factory=list)
    ambient_audio_events: List[AmbientAudioEvent] = field(default_factory=list)
    proximity_events: List[ProximityEvent] = field(default_factory=list)
    gaze_events: List[GazeEvent] = field(default_factory=list)
    posture_events: List[PostureEvent] = field(default_factory=list)
    object_events: List[ObjectEvent] = field(default_factory=list)
    
    # Layer 2 events (LLM inferred)
    behavioral_events: List[BehavioralEvent] = field(default_factory=list)
    interaction_events: List[InteractionEvent] = field(default_factory=list)
    context_events: List[ContextEvent] = field(default_factory=list)
    
    @property
    def layer1_events(self) -> List[EventBase]:
        """All raw sensor events."""
        events = (
            self.speech_events + 
            self.ambient_audio_events +
            self.proximity_events + 
            self.gaze_events +
            self.posture_events +
            self.object_events
        )
        return sorted(events, key=lambda e: e.timestamp)
    
    @property
    def layer2_events(self) -> List[EventBase]:
        """All LLM-inferred events."""
        events = (
            self.behavioral_events +
            self.interaction_events +
            self.context_events
        )
        return sorted(events, key=lambda e: e.timestamp)
    
    @property
    def all_events(self) -> List[EventBase]:
        """All events in chronological order."""
        return sorted(self.layer1_events + self.layer2_events, key=lambda e: e.timestamp)
    
    def get_child_actors(self) -> List[Actor]:
        """Get all child actors."""
        return [a for a in self.actors if a.role == ActorRole.CHILD]
    
    def get_adult_actors(self) -> List[Actor]:
        """Get all adult actors."""
        return [a for a in self.actors if a.role == ActorRole.ADULT]


# =============================================================================
# TYPE ALIASES FOR CONVENIENCE
# =============================================================================

Layer1Event = Union[
    SpeechEvent, AmbientAudioEvent, 
    ProximityEvent, GazeEvent, PostureEvent, ObjectEvent
]

Layer2Event = Union[
    BehavioralEvent, InteractionEvent, ContextEvent
]

Event = Union[Layer1Event, Layer2Event]
